Word embedding (word2vec) (trad : plongement lexical):
représentation des mots sous forme de vecteurs de nombre réels
mots sont alors représentés dans un espace vectoriel
hypothèse de harris : mots qui apparaissent dans des contextes similaires ont une signification similaire également
représentation vectorielle permet de comparer des mots entre eux (aspect phonétique, lexical, signification, etc ...) en mesurant l'angle entre les vecteurs
comparaison permet, dans l'espace vectoriel une algèbre propre (exemple : roi - homme + femme = reine)

Principe de construction :
Projection d'un ensemble de mots d'un vocabulaire de taille V
Vecteurs de ces mots ont une taille N (objectif est d'avoir un N réduit au maximum)
Utilisation de réseaux de neurones pour construire ces vecteurs entrainés sur des corpus immenses

mots sont d'abord codés en one-hot-encoding [0 0... 1 0...]

nn.Embedding est un tableau ou l'index correspond au mot du vocabulaire associé, et la valeur stockée à cet index est le vecteur du mot
Lors de la construction d'un embedding, il faut donc l'entrainer pour qu'il attribue les bonnes valeurs aux mots du dictionnaire